{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf8\n",
    "import requests, json\n",
    "import os\n",
    "import sqlite3\n",
    "from multiprocessing import Pool\n",
    "from time import time, sleep\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import functools\n",
    "import statistics\n",
    "\n",
    "import mwparserfromhell\n",
    "from googleapiclient import discovery, errors\n",
    "\n",
    "from settings import DATA_FOLDER, API_KEY\n",
    "\n",
    "DATABASE_PATH = os.path.join(DATA_FOLDER, 'WikiDB.db')\n",
    "TOXICITY_THRESHOLD = .25\n",
    "\n",
    "\n",
    "def handle_comment(comment_row):\n",
    "    print(comment_row)\n",
    "    article_id, text = comment_row[0], comment_row[1]\n",
    "    start_tox = time()\n",
    "    if toxicity_score(text) > TOXICITY_THRESHOLD:\n",
    "        toxic = True\n",
    "    else:\n",
    "        toxic = False\n",
    "    print(time() - start_tox)\n",
    "    return article_id, toxic\n",
    "\n",
    "\n",
    "def article_iterator_wrapper(comment_iterator):\n",
    "    comments = []\n",
    "    current_id = 1\n",
    "    for article_id, comment in comment_iterator:\n",
    "        if article_id == current_id:\n",
    "            comments.append(comment)\n",
    "        else:\n",
    "            yield current_id, comments\n",
    "            current_id = article_id\n",
    "            comments = [comment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toxicity_score(comment):\n",
    "    for i in range(100):\n",
    "        try:\n",
    "            if len(comment) > 3000:\n",
    "                return -1\n",
    "            service = discovery.build('commentanalyzer', 'v1alpha1', developerKey=API_KEY)\n",
    "            analyze_request = {\n",
    "              'comment': { 'text': comment },\n",
    "              'requestedAttributes': {'TOXICITY': {}}\n",
    "            }\n",
    "            try:\n",
    "                response = service.comments().analyze(body=analyze_request).execute()\n",
    "            except errors.HttpError:\n",
    "                return -1\n",
    "            return response[\"attributeScores\"][\"TOXICITY\"][\"summaryScore\"][\"value\"]\n",
    "        except:\n",
    "            print(\"Toxicity API unresponsive ; retrying in \" + str(4*i*i) + \"s\")\n",
    "            sleep(4*i*i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(DATABASE_PATH)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_index = json.load(open(os.path.join(DATA_FOLDER, '2110_final_index.json')))\n",
    "reverse_index = json.load(open(os.path.join(DATA_FOLDER, '2110_reverse_index.json')))\n",
    "edit_wars = json.load(open(os.path.join(DATA_FOLDER, 'edit_wars.json')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_list = []\n",
    "article_dict = {}\n",
    "article_iterator = cur.execute(\"SELECT * from article ORDER BY id\")\n",
    "for i, article in enumerate(article_iterator):\n",
    "    article_list.append(article[1])\n",
    "    article_dict[article[1]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_iterator = cur.execute(\"SELECT article_id, text from comment ORDER BY article_id\")\n",
    "start_time = time()\n",
    "total_comments = 0\n",
    "max_comments = 0\n",
    "comment_distribution = np.zeros(47628)\n",
    "comment_distribution_with_titles = []\n",
    "for i in range(47628):\n",
    "    comment_distribution_with_titles.append([])\n",
    "# pool = Pool(1)\n",
    "\n",
    "for i, (current_id, comments) in enumerate(article_iterator_wrapper(comment_iterator)):\n",
    "    talk_length = len(comments)\n",
    "    total_comments += talk_length\n",
    "    comment_distribution[talk_length - 1] += 1\n",
    "    comment_distribution_with_titles[talk_length - 1].append(article_list[current_id - 1])\n",
    "    if talk_length > max_comments:\n",
    "        max_comments = talk_length\n",
    "    if i % 1000 == 0:\n",
    "        print(time()-start_time)\n",
    "        print(i)\n",
    "\n",
    "\n",
    "print(max_comments)\n",
    "print(total_comments / float(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_distribution = np.cumsum(comment_distribution)\n",
    "\n",
    "cumulative_distribution[-1] - cumulative_distribution[150]\n",
    "\n",
    "cumulative_distribution[200] - cumulative_distribution[150]\n",
    "\n",
    "print(comment_distribution_with_titles[175])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_toxicity(row):\n",
    "    comment_id, text, previous_score = row[0], row[1], row[2]\n",
    "    if previous_score is not None:\n",
    "        return None, comment_id\n",
    "    cleaned_text = mwparserfromhell.parse(text).strip_code()\n",
    "    toxicity = toxicity_score(cleaned_text)\n",
    "    return toxicity, comment_id    \n",
    "    \n",
    "    \n",
    "def get_toxicity_for_cleanup(row):\n",
    "    comment_id, text = row[0], row[1]\n",
    "    return toxicity_score(mwparserfromhell.parse(text).strip_code()), comment_id\n",
    "    \n",
    "    \n",
    "def fetch_toxicity(articles_to_analyze):\n",
    "    start_time = time()\n",
    "    toxicity_dict = {}\n",
    "    ids_to_analyze = [article_dict.get(article, None) + 1 for article in articles_to_analyze if article_dict.get(article, None) is not None]\n",
    "    placeholder= '?'\n",
    "    placeholders= ', '.join(placeholder for item in ids_to_analyze)\n",
    "    query= 'SELECT id, text, toxicity FROM comment WHERE article_id IN (%s)' % placeholders\n",
    "    comment_iterator = cur.execute(query, ids_to_analyze)\n",
    "    comments_to_analyze = comment_iterator.fetchall()\n",
    "    print('total to analyze : ' + str(len(comments_to_analyze)))\n",
    "    start_loop = time()\n",
    "    print(start_loop - start_time)\n",
    "    \n",
    "    pool = Pool(3)\n",
    "    \n",
    "    for i, (toxicity, comment_id) in enumerate(pool.imap_unordered(get_toxicity, comments_to_analyze)):\n",
    "        if toxicity is not None:\n",
    "            toxicity_dict[comment_id] = toxicity\n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "            print(time() - start_loop)\n",
    "    print(i)\n",
    "    print(time() - start_loop)\n",
    "    \n",
    "    results = [(toxicity, comment_id) for comment_id, toxicity in toxicity_dict.items()]\n",
    "    query = \"UPDATE comment SET toxicity = ? WHERE id = ?\"\n",
    "    cur.executemany(query, results)\n",
    "    conn.commit()\n",
    "    \n",
    "    \n",
    "def fetch_toxicity_by_id(final_ids_to_analyze):\n",
    "    start_time = time()\n",
    "    toxicity_dict = {}\n",
    "    \n",
    "    placeholder= '?'\n",
    "    placeholders= ', '.join(placeholder for item in final_ids_to_analyze)\n",
    "    query= 'SELECT id FROM article WHERE talk_length < 200 AND final_id IN (%s)' % placeholders\n",
    "    comment_iterator = cur.execute(query, final_ids_to_analyze)\n",
    "    ids_to_analyze = [row[0] for row in comment_iterator]\n",
    "    \n",
    "    placeholder= '?'\n",
    "    placeholders= ', '.join(placeholder for item in ids_to_analyze)\n",
    "    query= 'SELECT id, text, toxicity FROM comment WHERE parent_id != 0 AND toxicity IS NULL AND article_id IN (%s)' % placeholders\n",
    "    comment_iterator = cur.execute(query, ids_to_analyze)\n",
    "    start_loop = time()\n",
    "    print(start_loop - start_time)\n",
    "    comments_to_analyze = comment_iterator.fetchall()\n",
    "    print('total to analyze : ' + str(len(comments_to_analyze)))\n",
    "    \n",
    "    pool = Pool(3)\n",
    "    \n",
    "    for i, (toxicity, comment_id) in enumerate(pool.imap_unordered(get_toxicity, comments_to_analyze)):\n",
    "        if toxicity is not None:\n",
    "            toxicity_dict[comment_id] = toxicity\n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "            print(time() - start_loop)\n",
    "    print(i)\n",
    "    print(time() - start_loop)\n",
    "    \n",
    "    results = [(toxicity, comment_id) for comment_id, toxicity in toxicity_dict.items()]\n",
    "    query = \"UPDATE comment SET toxicity = ? WHERE id = ?\"\n",
    "    cur.executemany(query, results)\n",
    "    conn.commit()\n",
    "    \n",
    "    \n",
    "def toxicity_iterator(comment_iterator):\n",
    "    comments = []\n",
    "    current_id = None\n",
    "    current_talk_length = None\n",
    "    for article_id, toxicity, talk_length in comment_iterator:\n",
    "        if article_id == current_id:\n",
    "            comments.append(toxicity)\n",
    "        elif current_id is None:\n",
    "            current_id = article_id\n",
    "            current_talk_length = talk_length\n",
    "            comments = [toxicity]\n",
    "        else:\n",
    "            yield current_id, comments, current_talk_length\n",
    "            current_talk_length = talk_length\n",
    "            current_id = article_id\n",
    "            comments = [toxicity]\n",
    "    \n",
    "\n",
    "def iterate_toxicity_by_id(final_ids_to_analyze):\n",
    "    start_time = time()\n",
    "    toxicity_dict = {}\n",
    "    \n",
    "    placeholder= '?'\n",
    "    placeholders= ', '.join(placeholder for item in final_ids_to_analyze)\n",
    "    query= 'SELECT final_id, toxicity, talk_length FROM comment INNER JOIN article ON comment.article_id = article.id WHERE toxicity IS NOT NULL AND parent_id != 0 AND final_id IN (%s) ORDER BY final_id' % placeholders\n",
    "    comment_iterator = cur.execute(query, final_ids_to_analyze)\n",
    "    start_loop = time()\n",
    "    print(start_loop - start_time)\n",
    "    \n",
    "    return toxicity_iterator(comment_iterator)\n",
    "\n",
    "\n",
    "def clean_toxicity():\n",
    "    start_time = time()\n",
    "    query= 'SELECT id, text FROM comment WHERE toxicity = -1'\n",
    "    comment_iterator = cur.execute(query)\n",
    "    comments_to_analyze = comment_iterator.fetchall()\n",
    "    print('total to analyze : ' + str(len(comments_to_analyze)))\n",
    "    toxicity_dict = {}\n",
    "    \n",
    "    problems = 0\n",
    "    pool = Pool(3)\n",
    "    \n",
    "    for i, (toxicity, comment_id) in enumerate(pool.imap_unordered(get_toxicity_for_cleanup, comments_to_analyze)):\n",
    "        if toxicity != -1:\n",
    "            problems += 1\n",
    "            toxicity_dict[comment_id] = toxicity\n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "            print(str(problems) + ' problems')\n",
    "            print(time() - start_time)\n",
    "    print(i)\n",
    "    print(time() - start_time)\n",
    "    \n",
    "    print(str(problems) + ' problems')\n",
    "    results = [(toxicity, comment_id) for comment_id, toxicity in toxicity_dict.items()]\n",
    "    query = \"UPDATE comment SET toxicity = ? WHERE id = ?\"\n",
    "    cur.executemany(query, results)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'SELECT article_id, toxicity FROM comment WHERE toxicity IS NOT NULL ORDER BY article_id'\n",
    "comment_iterator = cur.execute(query)\n",
    "toxic_comments = comment_iterator.fetchall()\n",
    "\n",
    "for article_id, toxicities in toxicity_iterator(toxic_comments):\n",
    "    print(article_list[article_id + 1])\n",
    "    print(len([value for value in toxicities if value > 0.25]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_wars_by_id = {final_index.get(article_title.strip(), None): value for article_title, value in edit_wars.items() if final_index.get(article_title.strip(), None) is not None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_wars = [int(item) for item, value in edit_wars_by_id.items() if value == 1]\n",
    "big_wars = [int(item) for item, value in edit_wars_by_id.items() if value != 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.119564771652222\n",
      "total to analyze : 25728\n",
      "0\n",
      "120.17145609855652\n",
      "1000\n",
      "213.73272156715393\n",
      "2000\n",
      "307.5810799598694\n",
      "3000\n",
      "401.2042136192322\n",
      "4000\n",
      "494.2507061958313\n",
      "5000\n",
      "587.5135655403137\n",
      "6000\n",
      "682.3595886230469\n",
      "7000\n",
      "775.1076378822327\n",
      "8000\n",
      "867.5104930400848\n",
      "9000\n",
      "960.6196150779724\n",
      "10000\n",
      "1052.2423701286316\n",
      "11000\n",
      "1145.041989326477\n",
      "12000\n",
      "1239.984529018402\n",
      "13000\n",
      "1331.3330903053284\n",
      "14000\n",
      "1423.7848436832428\n",
      "15000\n",
      "1515.0919589996338\n",
      "16000\n",
      "1609.2695860862732\n",
      "17000\n",
      "1701.4609694480896\n",
      "18000\n",
      "1793.3942289352417\n",
      "19000\n",
      "1886.3059031963348\n",
      "20000\n",
      "1976.8551437854767\n",
      "21000\n",
      "2068.565089225769\n",
      "22000\n",
      "2159.4753239154816\n",
      "23000\n",
      "2251.7271423339844\n",
      "24000\n",
      "2345.8577468395233\n",
      "25000\n",
      "2439.500079870224\n",
      "25727\n",
      "2506.8995604515076\n"
     ]
    }
   ],
   "source": [
    "fetch_toxicity_by_id(small_wars + big_wars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158.51154279708862\n",
      "5751\n"
     ]
    }
   ],
   "source": [
    "active_edit_wars_by_id = []\n",
    "for i, (final_id, toxicities, talk_length) in enumerate(iterate_toxicity_by_id(small_wars + big_wars)):\n",
    "    if len(toxicities) > 6 or talk_length > 200 :\n",
    "        active_edit_wars_by_id.append(final_id)\n",
    "print(len(active_edit_wars_by_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9237\n"
     ]
    }
   ],
   "source": [
    "query = 'SELECT final_id FROM article WHERE talk_length > 200'\n",
    "article_iterator = cur.execute(query)\n",
    "big_articles = set(row[0] for row in article_iterator.fetchall())\n",
    "print(len(big_articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'big_articles' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-94f20a552048>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfinal_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbig_articles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactive_edit_wars_by_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_FOLDER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'2601_conflicts.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'big_articles' is not defined"
     ]
    }
   ],
   "source": [
    "final_dataset = big_articles.union(active_edit_wars_by_id)\n",
    "print(len(final_dataset))\n",
    "json.dump(list(final_dataset), open(os.path.join(DATA_FOLDER, '2601_conflicts.json'), 'w'), indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'SELECT id, text, toxicity FROM comment INNER JOIN article ON comment.article_id = article.id WHERE parent_id != 0 AND talk_length < 200 AND toxicity IS NOT NULL ORDER BY final_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_dataset = json.load(open(os.path.join(DATA_FOLDER, '2601_conflicts.json')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "placeholder= '?'\n",
    "placeholders= ', '.join(placeholder for item in big_dataset)\n",
    "query = 'SELECT user_id, final_id from comment INNER JOIN article ON comment.article_id = article.id WHERE final_id IN (%s) ORDER BY final_id' % placeholders\n",
    "comment_iterator = cur.execute(query, big_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_users = {final_id: [] for final_id in big_dataset}\n",
    "for user_id, final_id in comment_iterator:\n",
    "    article_users[final_id].append(user_id)\n",
    "json.dump(article_users, open(os.path.join(DATA_FOLDER, '2601_article_users.json'), 'w'), indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_users = json.load(open(os.path.join(DATA_FOLDER, '2601_article_users.json')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_users = {user for article_list in article_users.values() for user in article_list }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "531795"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6625525"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(len(article_list) for article_list in article_users.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_articles = {user: [] for user in all_users}\n",
    "for article, users in article_users.items():\n",
    "    for user in users:\n",
    "        users_articles[user].append(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7829,\n",
       " 1807,\n",
       " 1635,\n",
       " 1151,\n",
       " 873,\n",
       " 858,\n",
       " 812,\n",
       " 730,\n",
       " 724,\n",
       " 664,\n",
       " 646,\n",
       " 623,\n",
       " 605,\n",
       " 594,\n",
       " 590,\n",
       " 588,\n",
       " 538,\n",
       " 532,\n",
       " 525,\n",
       " 518,\n",
       " 514,\n",
       " 512,\n",
       " 509,\n",
       " 494,\n",
       " 487,\n",
       " 477,\n",
       " 475,\n",
       " 459,\n",
       " 459,\n",
       " 459,\n",
       " 459,\n",
       " 457,\n",
       " 456,\n",
       " 450,\n",
       " 443,\n",
       " 440,\n",
       " 440,\n",
       " 438,\n",
       " 438,\n",
       " 433,\n",
       " 432,\n",
       " 431,\n",
       " 428,\n",
       " 428,\n",
       " 424,\n",
       " 422,\n",
       " 418,\n",
       " 418,\n",
       " 410,\n",
       " 409,\n",
       " 399,\n",
       " 396,\n",
       " 393,\n",
       " 389,\n",
       " 388,\n",
       " 381,\n",
       " 381,\n",
       " 376,\n",
       " 376,\n",
       " 375,\n",
       " 373,\n",
       " 369,\n",
       " 369,\n",
       " 368,\n",
       " 367,\n",
       " 359,\n",
       " 353,\n",
       " 353,\n",
       " 352,\n",
       " 344,\n",
       " 343,\n",
       " 342,\n",
       " 342,\n",
       " 338,\n",
       " 337,\n",
       " 335,\n",
       " 334,\n",
       " 328,\n",
       " 324,\n",
       " 324,\n",
       " 321,\n",
       " 320,\n",
       " 319,\n",
       " 319,\n",
       " 318,\n",
       " 318,\n",
       " 317,\n",
       " 316,\n",
       " 315,\n",
       " 314,\n",
       " 314,\n",
       " 311,\n",
       " 311,\n",
       " 311,\n",
       " 306,\n",
       " 304,\n",
       " 303,\n",
       " 303,\n",
       " 301,\n",
       " 301,\n",
       " 300,\n",
       " 298,\n",
       " 295,\n",
       " 295,\n",
       " 294,\n",
       " 294,\n",
       " 294,\n",
       " 292,\n",
       " 290,\n",
       " 289,\n",
       " 287,\n",
       " 287,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 280,\n",
       " 280,\n",
       " 280,\n",
       " 280,\n",
       " 278,\n",
       " 276,\n",
       " 275,\n",
       " 275,\n",
       " 274,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 272,\n",
       " 272,\n",
       " 268,\n",
       " 265,\n",
       " 264,\n",
       " 263,\n",
       " 263,\n",
       " 263,\n",
       " 262,\n",
       " 261,\n",
       " 261,\n",
       " 259,\n",
       " 259,\n",
       " 258,\n",
       " 258,\n",
       " 258,\n",
       " 254,\n",
       " 253,\n",
       " 251,\n",
       " 250,\n",
       " 249,\n",
       " 249,\n",
       " 248,\n",
       " 247,\n",
       " 247,\n",
       " 245,\n",
       " 245,\n",
       " 244,\n",
       " 243,\n",
       " 242,\n",
       " 241,\n",
       " 241,\n",
       " 240,\n",
       " 239,\n",
       " 238,\n",
       " 238,\n",
       " 238,\n",
       " 238,\n",
       " 237,\n",
       " 237,\n",
       " 235,\n",
       " 235,\n",
       " 235,\n",
       " 235,\n",
       " 233,\n",
       " 232,\n",
       " 232,\n",
       " 231,\n",
       " 231,\n",
       " 230,\n",
       " 230,\n",
       " 229,\n",
       " 229,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 227,\n",
       " 226,\n",
       " 226,\n",
       " 226,\n",
       " 226,\n",
       " 226,\n",
       " 225,\n",
       " 225,\n",
       " 224,\n",
       " 224,\n",
       " 224,\n",
       " 223,\n",
       " 223,\n",
       " 222,\n",
       " 222,\n",
       " 222,\n",
       " 221,\n",
       " 221,\n",
       " 220,\n",
       " 219,\n",
       " 217,\n",
       " 217,\n",
       " 217,\n",
       " 216,\n",
       " 216,\n",
       " 216,\n",
       " 216,\n",
       " 215,\n",
       " 215,\n",
       " 214,\n",
       " 214,\n",
       " 214,\n",
       " 213,\n",
       " 213,\n",
       " 213,\n",
       " 213,\n",
       " 213,\n",
       " 212,\n",
       " 212,\n",
       " 211,\n",
       " 211,\n",
       " 211,\n",
       " 211,\n",
       " 210,\n",
       " 209,\n",
       " 209,\n",
       " 209,\n",
       " 209,\n",
       " 209,\n",
       " 209,\n",
       " 208,\n",
       " 207,\n",
       " 207,\n",
       " 206,\n",
       " 206,\n",
       " 206,\n",
       " 206,\n",
       " 205,\n",
       " 205,\n",
       " 204,\n",
       " 204,\n",
       " 204,\n",
       " 202,\n",
       " 202,\n",
       " 202,\n",
       " 202,\n",
       " 202,\n",
       " 201,\n",
       " 201,\n",
       " 201,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 199,\n",
       " 198,\n",
       " 197,\n",
       " 197,\n",
       " 197,\n",
       " 196,\n",
       " 196,\n",
       " 195,\n",
       " 195,\n",
       " 195,\n",
       " 195,\n",
       " 195,\n",
       " 195,\n",
       " 194,\n",
       " 194,\n",
       " 194,\n",
       " 194,\n",
       " 193,\n",
       " 193,\n",
       " 193,\n",
       " 192,\n",
       " 190,\n",
       " 188,\n",
       " 188,\n",
       " 188,\n",
       " 188,\n",
       " 188,\n",
       " 187,\n",
       " 187,\n",
       " 187,\n",
       " 187,\n",
       " 187,\n",
       " 186,\n",
       " 186,\n",
       " 186,\n",
       " 186,\n",
       " 186,\n",
       " 185,\n",
       " 183,\n",
       " 183,\n",
       " 183,\n",
       " 182,\n",
       " 182,\n",
       " 182,\n",
       " 182,\n",
       " 181,\n",
       " 181,\n",
       " 181,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 179,\n",
       " 179,\n",
       " 178,\n",
       " 178,\n",
       " 178,\n",
       " 178,\n",
       " 178,\n",
       " 178,\n",
       " 178,\n",
       " 177,\n",
       " 177,\n",
       " 176,\n",
       " 176,\n",
       " 175,\n",
       " 174,\n",
       " 174,\n",
       " 174,\n",
       " 174,\n",
       " 174,\n",
       " 174,\n",
       " 173,\n",
       " 173,\n",
       " 173,\n",
       " 172,\n",
       " 172,\n",
       " 172,\n",
       " 171,\n",
       " 171,\n",
       " 171,\n",
       " 171,\n",
       " 171,\n",
       " 170,\n",
       " 170,\n",
       " 169,\n",
       " 169,\n",
       " 169,\n",
       " 169,\n",
       " 169,\n",
       " 168,\n",
       " 168,\n",
       " 168,\n",
       " 168,\n",
       " 167,\n",
       " 167,\n",
       " 167,\n",
       " 167,\n",
       " 166,\n",
       " 166,\n",
       " 166,\n",
       " 166,\n",
       " 165,\n",
       " 165,\n",
       " 165,\n",
       " 164,\n",
       " 164,\n",
       " 164,\n",
       " 164,\n",
       " 163,\n",
       " 163,\n",
       " 163,\n",
       " 163,\n",
       " 162,\n",
       " 162,\n",
       " 162,\n",
       " 162,\n",
       " 162,\n",
       " 161,\n",
       " 161,\n",
       " 161,\n",
       " 161,\n",
       " 161,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 159,\n",
       " 159,\n",
       " 159,\n",
       " 159,\n",
       " 159,\n",
       " 158,\n",
       " 158,\n",
       " 158,\n",
       " 158,\n",
       " 158,\n",
       " 158,\n",
       " 157,\n",
       " 157,\n",
       " 157,\n",
       " 157,\n",
       " 157,\n",
       " 157,\n",
       " 157,\n",
       " 156,\n",
       " 156,\n",
       " 156,\n",
       " 156,\n",
       " 155,\n",
       " 155,\n",
       " 155,\n",
       " 155,\n",
       " 155,\n",
       " 155,\n",
       " 154,\n",
       " 154,\n",
       " 154,\n",
       " 154,\n",
       " 154,\n",
       " 154,\n",
       " 153,\n",
       " 153,\n",
       " 153,\n",
       " 152,\n",
       " 152,\n",
       " 152,\n",
       " 151,\n",
       " 151,\n",
       " 151,\n",
       " 151,\n",
       " 151,\n",
       " 151,\n",
       " 150,\n",
       " 150,\n",
       " 150,\n",
       " 150,\n",
       " 150,\n",
       " 150,\n",
       " 150,\n",
       " 150,\n",
       " 150,\n",
       " 149,\n",
       " 149,\n",
       " 149,\n",
       " 149,\n",
       " 149,\n",
       " 149,\n",
       " 149,\n",
       " 149,\n",
       " 149,\n",
       " 149,\n",
       " 148,\n",
       " 148,\n",
       " 148,\n",
       " 148,\n",
       " 147,\n",
       " 147,\n",
       " 147,\n",
       " 147,\n",
       " 147,\n",
       " 147,\n",
       " 146,\n",
       " 146,\n",
       " 146,\n",
       " 146,\n",
       " 146,\n",
       " 146,\n",
       " 146,\n",
       " 145,\n",
       " 145,\n",
       " 145,\n",
       " 145,\n",
       " 144,\n",
       " 144,\n",
       " 143,\n",
       " 143,\n",
       " 143,\n",
       " 143,\n",
       " 143,\n",
       " 143,\n",
       " 143,\n",
       " 142,\n",
       " 142,\n",
       " 142,\n",
       " 142,\n",
       " 142,\n",
       " 142,\n",
       " 142,\n",
       " 142,\n",
       " 142,\n",
       " 142,\n",
       " 142,\n",
       " 141,\n",
       " 141,\n",
       " 141,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 138,\n",
       " 138,\n",
       " 138,\n",
       " 138,\n",
       " 138,\n",
       " 138,\n",
       " 138,\n",
       " 138,\n",
       " 138,\n",
       " 137,\n",
       " 137,\n",
       " 137,\n",
       " 137,\n",
       " 137,\n",
       " 137,\n",
       " 137,\n",
       " 137,\n",
       " 137,\n",
       " 136,\n",
       " 136,\n",
       " 136,\n",
       " 136,\n",
       " 136,\n",
       " 136,\n",
       " 136,\n",
       " 135,\n",
       " 135,\n",
       " 135,\n",
       " 135,\n",
       " 134,\n",
       " 134,\n",
       " 134,\n",
       " 134,\n",
       " 134,\n",
       " 134,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 131,\n",
       " 131,\n",
       " 131,\n",
       " 131,\n",
       " 131,\n",
       " 131,\n",
       " 131,\n",
       " 131,\n",
       " 130,\n",
       " 130,\n",
       " 130,\n",
       " 130,\n",
       " 130,\n",
       " 130,\n",
       " 130,\n",
       " 130,\n",
       " 130,\n",
       " 130,\n",
       " 130,\n",
       " 130,\n",
       " 129,\n",
       " 129,\n",
       " 129,\n",
       " 129,\n",
       " 129,\n",
       " 129,\n",
       " 129,\n",
       " 129,\n",
       " 129,\n",
       " 129,\n",
       " 129,\n",
       " 129,\n",
       " 129,\n",
       " 129,\n",
       " 129,\n",
       " 129,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 127,\n",
       " 127,\n",
       " 127,\n",
       " 127,\n",
       " 127,\n",
       " 126,\n",
       " 126,\n",
       " 126,\n",
       " 126,\n",
       " 126,\n",
       " 126,\n",
       " 126,\n",
       " 125,\n",
       " 125,\n",
       " 125,\n",
       " 125,\n",
       " 125,\n",
       " 125,\n",
       " 125,\n",
       " 125,\n",
       " 125,\n",
       " 125,\n",
       " 124,\n",
       " 124,\n",
       " 124,\n",
       " 124,\n",
       " 124,\n",
       " 123,\n",
       " 123,\n",
       " 123,\n",
       " 123,\n",
       " 123,\n",
       " 123,\n",
       " 123,\n",
       " 123,\n",
       " 123,\n",
       " 123,\n",
       " 123,\n",
       " 123,\n",
       " 123,\n",
       " 123,\n",
       " 123,\n",
       " 123,\n",
       " 123,\n",
       " 123,\n",
       " 123,\n",
       " 123,\n",
       " 122,\n",
       " 122,\n",
       " 122,\n",
       " 122,\n",
       " 122,\n",
       " 122,\n",
       " 122,\n",
       " 121,\n",
       " 121,\n",
       " 121,\n",
       " 121,\n",
       " 121,\n",
       " 121,\n",
       " 121,\n",
       " 121,\n",
       " 121,\n",
       " 121,\n",
       " 121,\n",
       " 121,\n",
       " 121,\n",
       " 121,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 119,\n",
       " 119,\n",
       " 119,\n",
       " 119,\n",
       " 119,\n",
       " 119,\n",
       " 119,\n",
       " 119,\n",
       " 119,\n",
       " 119,\n",
       " 119,\n",
       " 119,\n",
       " 119,\n",
       " 119,\n",
       " 119,\n",
       " 119,\n",
       " 118,\n",
       " 118,\n",
       " 118,\n",
       " 118,\n",
       " 118,\n",
       " 118,\n",
       " 118,\n",
       " 117,\n",
       " 117,\n",
       " 117,\n",
       " 117,\n",
       " 117,\n",
       " 117,\n",
       " 117,\n",
       " 117,\n",
       " 117,\n",
       " 117,\n",
       " 117,\n",
       " 117,\n",
       " 116,\n",
       " 116,\n",
       " 116,\n",
       " 116,\n",
       " 116,\n",
       " 116,\n",
       " 116,\n",
       " 116,\n",
       " 115,\n",
       " 115,\n",
       " 115,\n",
       " 115,\n",
       " 115,\n",
       " 115,\n",
       " 115,\n",
       " 115,\n",
       " 115,\n",
       " 115,\n",
       " 115,\n",
       " 115,\n",
       " 115,\n",
       " 114,\n",
       " 114,\n",
       " 114,\n",
       " 114,\n",
       " 114,\n",
       " 114,\n",
       " 114,\n",
       " 114,\n",
       " 114,\n",
       " 114,\n",
       " 113,\n",
       " 113,\n",
       " 113,\n",
       " 113,\n",
       " 113,\n",
       " 113,\n",
       " 113,\n",
       " 113,\n",
       " 113,\n",
       " 113,\n",
       " 113,\n",
       " 113,\n",
       " 113,\n",
       " 113,\n",
       " 113,\n",
       " 113,\n",
       " 113,\n",
       " 112,\n",
       " 112,\n",
       " 112,\n",
       " 112,\n",
       " 112,\n",
       " 112,\n",
       " 112,\n",
       " 112,\n",
       " 112,\n",
       " 112,\n",
       " 112,\n",
       " 112,\n",
       " 111,\n",
       " 111,\n",
       " 111,\n",
       " 111,\n",
       " 111,\n",
       " 111,\n",
       " 111,\n",
       " 111,\n",
       " 111,\n",
       " 111,\n",
       " 111,\n",
       " 110,\n",
       " 110,\n",
       " 110,\n",
       " 110,\n",
       " 110,\n",
       " 110,\n",
       " 110,\n",
       " 110,\n",
       " 109,\n",
       " 109,\n",
       " 109,\n",
       " 109,\n",
       " 109,\n",
       " 109,\n",
       " 109,\n",
       " 108,\n",
       " 108,\n",
       " 108,\n",
       " 108,\n",
       " 108,\n",
       " 108,\n",
       " 108,\n",
       " 108,\n",
       " 108,\n",
       " 108,\n",
       " 108,\n",
       " 108,\n",
       " 107,\n",
       " 107,\n",
       " 107,\n",
       " 107,\n",
       " 107,\n",
       " 107,\n",
       " 107,\n",
       " 107,\n",
       " 107,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 105,\n",
       " 105,\n",
       " 105,\n",
       " 105,\n",
       " 105,\n",
       " 105,\n",
       " 105,\n",
       " 105,\n",
       " 105,\n",
       " 105,\n",
       " 105,\n",
       " 105,\n",
       " 105,\n",
       " 105,\n",
       " 105,\n",
       " 105,\n",
       " 105,\n",
       " 105,\n",
       " 104,\n",
       " 104,\n",
       " 104,\n",
       " 104,\n",
       " 104,\n",
       " 104,\n",
       " 104,\n",
       " 104,\n",
       " 104,\n",
       " 104,\n",
       " 104,\n",
       " 104,\n",
       " 104,\n",
       " 104,\n",
       " 104,\n",
       " 104,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 102,\n",
       " 102,\n",
       " 102,\n",
       " 102,\n",
       " 102,\n",
       " 102,\n",
       " 102,\n",
       " 102,\n",
       " 102,\n",
       " 102,\n",
       " 102,\n",
       " 102,\n",
       " 102,\n",
       " 102,\n",
       " 102,\n",
       " 102,\n",
       " 102,\n",
       " 102,\n",
       " 102,\n",
       " 102,\n",
       " 102,\n",
       " 102,\n",
       " 102,\n",
       " 101,\n",
       " 101,\n",
       " 101,\n",
       " 101,\n",
       " 101,\n",
       " 101,\n",
       " 101,\n",
       " 101,\n",
       " 101,\n",
       " 101,\n",
       " 101,\n",
       " 101,\n",
       " 101,\n",
       " 101,\n",
       " 101,\n",
       " 101,\n",
       " 101,\n",
       " 101,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 99,\n",
       " 99,\n",
       " 99,\n",
       " 99,\n",
       " 99,\n",
       " 99,\n",
       " 99,\n",
       " 99,\n",
       " 99,\n",
       " 99,\n",
       " 99,\n",
       " 99,\n",
       " 99,\n",
       " 99,\n",
       " 99,\n",
       " 98,\n",
       " 98,\n",
       " 98,\n",
       " 98,\n",
       " 98,\n",
       " 98,\n",
       " 98,\n",
       " 98,\n",
       " 98,\n",
       " 98,\n",
       " 98,\n",
       " 98,\n",
       " 98,\n",
       " 98,\n",
       " 98,\n",
       " 98,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " ...]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([len(set(articles)) for articles in users_articles.values()], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bots = [user for user in users_articles if len(set(users_articles[user])) > 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_articles_no_bots = {user: articles for user, articles in users_articles.items() if user not in bots}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_articles = statistics.median((len(set(articles)) for articles in users_articles_no_bots.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(median_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "placeholder= '?'\n",
    "placeholders= ', '.join(placeholder for item in big_dataset)\n",
    "query = 'SELECT user_id, final_id from comment INNER JOIN article ON comment.article_id = article.id WHERE toxicity > 0.25 AND parent_id != 0 AND final_id IN (%s) ORDER BY final_id' % placeholders\n",
    "comment_iterator = cur.execute(query, big_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11539\n",
      "29501\n"
     ]
    }
   ],
   "source": [
    "toxic_article_users = {final_id: [] for final_id in big_dataset}\n",
    "for user_id, final_id in comment_iterator:\n",
    "    toxic_article_users[final_id].append(user_id)\n",
    "toxic_users = {user for article_list in toxic_article_users.values() for user in article_list }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_article_users = {k: v for k, v in toxic_article_users.items() if len(v) != 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11539\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "print(len(toxic_users))\n",
    "print(statistics.median(len(set(article_list)) for article_list in toxic_article_users.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
